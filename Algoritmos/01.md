# Algoritmos

## Principios para Analizar Algoritmos

### Principio #1

- Análisis pesimista: el tiempo de ejecución que calculemos vale para cualquier input posible si calculamos el peor caso posible de largo n. A diferencia de los otros 2, no requiere conocer el dominio de la rutina.

- Caso promedio: Asumir la frecuencia de ciertos inputs, analizar tiempo promedio.

- Benchmark: Convención de inputs.

### Principio #2

No prestar mucha atención a los factores constantes.

- Más fácil.

- Depende de la implementación, compilador, arquitectura, etc.

- Muy poca diferencia para predecir performance.

### Principio #3

Análisis asintótico: Concentrarse en tiempo de ejecución para inputs *grandes* de n.

Ejemplo: $6n\log_2(n)+6n$ es "mejor que" $1/2n^2$, es decir, es mejor para valores suficientemente grandes de n.

## Análisis Asintótico

Vocabulario usado para el diseño y análisis de algoritmos. Nos permite simplificar detalles de implementación, pero complejizar comparaciones predictivas entre algoritmos distintos, especialmente con inputs grandes.

En resumen: Buscamos *suprimir factores constantes y términos de nivel inferior*.

Ejemplo: Tratamos como equivalentes a $6n \log_2(n)$ + 6n con n log n. Entonces, Merge Sort es un algoritmos O(n log(n)), donde n es el tamaño del input.

## Notación $O()$

Definición: $T(n)=O(f(n))$ si y sólo si existen constantes $c,n_0>0$ tal que $T(n)\le c*f(n)$, para todo $n\ge n_0$ (con $c,n_0$ independiente de $n$).

## Propiedades/Ejemplos

1) Si $T(n)$ es un polinomio de grado $k$, lo único que nos importa es ese n de grado más alto: $T(n)=O(n^k)$. Es decir, descartamos las constantes.

2) Para todo $k>=1$, $n^k$ no es $O(N^{k-1})$.

## Notación $\Omega$ y $\Theta$

- Omega: $T(n)=\Omega(f(n))$ si y sólo si existen constantes $c,n0$ tales que $T(n)\ge c\cdot f(n)\forall n\ge n_0$.

- Theta: $T(n)=\Theta (f(n))$ si y sólo si $T(n)=O(f(n))$ y $T(n)=\Omega (f(n))$. Equivalente: Existen constantes $c_1,c_2,n_0$ tales que $c_1f(n)\le T(n)\le c_2f(n)$.

## Notación $o$

Definición: $T(n)=o(f(n))$ si y sólo si para *todas* las constantes $c>0$, existe una constante $n_0$ tal que $T(n)\le c\cdot f(n),\forall n\ge n_0$.

## Divide and Conquer

1) Dividir el problema en parte menores (A veces implica acotar input, a veces es sólo conceptual).

2) Aplicar recursión.

3) Recombinar la solución de los subproblemas.

## Problema de contar inversiones

if n = 1 return 0
else
	x = count (1st half of A, n/2)
	y = count (2nd half ")
	z = count split inv (A, n)

- Split inversions: Merge en Mergesort ya las descubre. Si no hay SI, Merge en mergesort es trivial => si copia algo de C antes de B, esa era una SI (Salvo que B ya esté vacío). Las SI del elemento Y de C son exactamente los valores restantes en B cuando se copia Y.

## Algoritmo de Strassen

- Multiplicación de matrices por definición: $O(n^3)$. Por recursión simple también.

## Algoritmo del par más cercano

- Input: set de puntos {x, y}

## Master Method

### Ejemplo: Multiplicación de Enteros

T(n) = Máximo número de operaciones que el algoritmo necesita para multiplicar dos números de n dígitos.

Recurrencia: expresar T(n) en términos de llamadas recursivas.

### Requisitos

- Todos los subproblemas tienen tamaño igual.

### Formato

1) Caso base: T(n) <= constante para n lo suficientemente pequeño.

2) Para todo n mayor: $T(n)<=aT(n/b) + O(n^d)$, con a = n° de subproblemas (>= 1), b = factor por el que se encoje el input (> 1), d = exponente in tiempo de suma al combinar (>= 0) (los 3 *independientes* de n).

### Definición

$T(n) = \left\{\begin{array}{lr}
        O(n^d\log n), & \text{si } a=b^d\\
        O(n^d), & \text{si } a<b^d\\
        O(n^{\log_ba}), & \text{si } a>b^d
        \end{array}\right\}$

(Nota: La base del primer log no importa, porque la diferencia entre dos bases es un factor constante, y la notación $O()$ suprime las constantes.)

### Interpretación

$Trabajo Total\leq cn^d*\sum_{j=0}^{log_bn}[a/b^d]^j$

- $j$ = subproblema.

- $a$ = tasa del subproblema (RSP)

- $b^d$ = tasa de encojimiento del trabajo por subproblema (RWS)

1) $RSP = RWS \Rightarrow$ Misma cantidad de trabajo en cada nivel (ej. MergeSort) (Esperamos $O(n^d\log n$))
2) $RSP<RWS \Rightarrow$ Menos trabajo cada nivel => Mayoría del trabajo en la raíz (esperamos ($O(n^d)$)
3) $RSP > RWS \Rightarrow$ Más trabajo por nivel => Mayoría del trabajo en las hojas (Esperamos $O(n°hojas)=O(a^{log_b n})$) ($a^{log_b n}=n^{log_b a}$)