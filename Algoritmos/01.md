# Algoritmos

## Principios para Analizar Algoritmos

### Principio #1

- Análisis pesimista: el tiempo de ejecución que calculemos vale para cualquier input posible si calculamos el peor caso posible de largo n. A diferencia de los otros 2, no requiere conocer el dominio de la rutina.

- Caso promedio: Asumir la frecuencia de ciertos inputs, analizar tiempo promedio.

- Benchmark: Convención de inputs.

### Principio #2

No prestar mucha atención a los factores constantes.

- Más fácil.

- Depende de la implementación, compilador, arquitectura, etc.

- Muy poca diferencia para predecir performance.

### Principio #3

Análisis asintótico: Concentrarse en tiempo de ejecución para inputs *grandes* de n.

Ejemplo: $6n\log_2(n)+6n$ es "mejor que" $1/2n^2$, es decir, es mejor para valores suficientemente grandes de n.

## Análisis Asintótico

Vocabulario usado para el diseño y análisis de algoritmos. Nos permite simplificar detalles de implementación, pero complejizar comparaciones predictivas entre algoritmos distintos, especialmente con inputs grandes.

En resumen: Buscamos *suprimir factores constantes y términos de nivel inferior*.

Ejemplo: Tratamos como equivalentes a $6n \log_2(n)$ + 6n con n log n. Entonces, Merge Sort es un algoritmos O(n log(n)), donde n es el tamaño del input.

## Notación $O()$

Definición: $T(n)=O(f(n))$ si y sólo si existen constantes $c,n_0>0$ tal que $T(n)\le c*f(n)$, para todo $n\ge n_0$ (con $c,n_0$ independiente de $n$).

## Propiedades/Ejemplos

1) Si $T(n)$ es un polinomio de grado $k$, lo único que nos importa es ese n de grado más alto: $T(n)=O(n^k)$. Es decir, descartamos las constantes.

2) Para todo $k>=1$, $n^k$ no es $O(N^{k-1})$.

## Notación $\Omega$ y $\Theta$

- Omega: $T(n)=\Omega(f(n))$ si y sólo si existen constantes $c,n0$ tales que $T(n)\ge c\cdot f(n)\forall n\ge n_0$.

- Theta: $T(n)=\Theta (f(n))$ si y sólo si $T(n)=O(f(n))$ y $T(n)=\Omega (f(n))$. Equivalente: Existen constantes $c_1,c_2,n_0$ tales que $c_1f(n)\le T(n)\le c_2f(n)$.

## Notación $o$

Definición: $T(n)=o(f(n))$ si y sólo si para *todas* las constantes $c>0$, existe una constante $n_0$ tal que $T(n)\le c\cdot f(n),\forall n\ge n_0$.

## Divide and Conquer

1) Dividir el problema en parte menores (A veces implica acotar input, a veces es sólo conceptual).

2) Aplicar recursión.

3) Recombinar la solución de los subproblemas.

## Problema de contar inversiones

if n = 1 return 0
else
	x = count (1st half of A, n/2)
	y = count (2nd half ")
	z = count split inv (A, n)

- Split inversions: Merge en Mergesort ya las descubre. Si no hay SI, Merge en mergesort es trivial => si copia algo de C antes de B, esa era una SI (Salvo que B ya esté vacío). Las SI del elemento Y de C son exactamente los valores restantes en B cuando se copia Y.

## Algoritmo de Strassen

- Multiplicación de matrices por definición: $O(n^3)$. Por recursión simple también.
- Strassen reduce el algoritmo de 8 recursiones a 7, por lo que el tiempo es $O(n^{log_2 7}) = O(n^{\approx 2.8})$.

## Algoritmo del par más cercano

- Input: lista de puntos $\{x, y\}$

# Master Method

### Ejemplo: Multiplicación de Enteros

T(n) = Máximo número de operaciones que el algoritmo necesita para multiplicar dos números de n dígitos.

Recurrencia: expresar T(n) en términos de llamadas recursivas.

### Requisitos

- Todos los subproblemas tienen tamaño igual.

### Formato

1) Caso base: T(n) <= constante para n lo suficientemente pequeño.

2) Para todo n mayor: $T(n)<=aT(n/b) + O(n^d)$, con a = n° de subproblemas (>= 1), b = factor por el que se encoje el input (> 1), d = exponente in tiempo de suma al combinar (>= 0) (los 3 *independientes* de n).

### Definición

$T(n) = \left\{\begin{array}{lr}
        O(n^d\log n), & \text{si } a=b^d\\
        O(n^d), & \text{si } a<b^d\\
        O(n^{\log_ba}), & \text{si } a>b^d
        \end{array}\right\}$

(Nota: La base del primer log no importa, porque la diferencia entre dos bases es un factor constante, y la notación $O()$ suprime las constantes.)

### Interpretación

$Trabajo Total\leq cn^d*\sum_{j=0}^{log_bn}[a/b^d]^j$

- $j$ = subproblema.

- $a$ = tasa del subproblema (RSP)

- $b^d$ = tasa de encojimiento del trabajo por subproblema (RWS)

1) $RSP = RWS \Rightarrow$ Misma cantidad de trabajo en cada nivel (ej. MergeSort) (Esperamos $O(n^d\log n$))
2) $RSP<RWS \Rightarrow$ Menos trabajo cada nivel => Mayoría del trabajo en la raíz (esperamos ($O(n^d)$)
3) $RSP > RWS \Rightarrow$ Más trabajo por nivel => Mayoría del trabajo en las hojas (Esperamos $O(n°hojas)=O(a^{log_b n})$) ($a^{log_b n}=n^{log_b a}$)

# QuickSort

Input: Array de n números sin ordenar.

Idea clave: Array partición alrededor de un elemento pivot.

- Elegir elemento de array (pivot)
- Ordenar elementos menores a la izquierda
- Ordenar elementos mayores a la derecha

### Particiones
1) Tiempo lineal $O(n)$, sin consumir más memoria
2) Reduce tamaño del problema (D&C)

Idea general:
- Un solo scan por el array
- Invariante: todo n que ya recorrimos está particionado
- Dos indices: $j = split \text{ escaneado/no escaneado}, i = split < p, >p$

### Descripción

```
QuickSort (array A, largo n)
        if n == 1 return
        p = ElegirPivot(A, n)
        Particionar(A, p)
        QuickSort( A[begin:p] )
        QuickSort( A[p:end] )
```

### Pivots

El tiempo de ejecución de QuickSort depende de la *calidad* del pivot. Un pivot es de buena calidad si divide el array en dos subproblemas de tamaño lo más igual posible.

Idea: Elegir pivot al azar. Sacar 25/75 split siempre alcanzaría para llegar a O(n log n). Eligiendo al azar lo logramos 1/2 veces.

## Tiempo de Ejecución Promedio de QS

Para cada array input de largo n, el tiempo de ejecución promedio con pivots al azar es $O(n \log n)$. Por "promedio" nos referimos sólo a que el algoritmo toma una decisión al azar, no la data. Aplica para todo input.

- Array $A$ de largo $n$.

- Sample space $\Omega$: todos los resultados posibles de las decisiones al azar de QS (i.e. todas las secuencias de pivots posibles).

- Variable aleatoria clave: para $\sigma \in \Omega, C(\sigma )=$ número de comparaciones entre dos elementos del input hechas por QuickSort (dada una secuencia $\sigma$.)

- Lema: El tiempo de ejecución de QS está dominado por sus comparaciones.

Meta: Probar que $E[C]=O(n \log n)$.

- $z_i=$ iésimo elemento *más pequeño* de $A$.
- Para $\sigma\in \Omega$, indices $i<j$, $X_{ij}(\sigma )=0\lor 1$ (número de veces que se comparan $z_i,z_j$ en QuickSort con pivot secuencia $\sigma$).
- Por lo tanto, $\forall \sigma , C(\sigma )=\sum_{i=1}^{n-1} \sum_{j=i+1}^n X_{ij}(\sigma )$
- Por linealidad de expectativas: $E[C]=\sum_{i=1}^{n-1} \sum_{j=i+1}^n E[X_{ij}]$
- También $E[X_{ij}]=Pr[X_{ij}=1]$
- Finalmente: $E[C]=\sum_{i=1}^{n-1} \sum_{j=i+1}^n Pr[z_i,z_j \text{ se comparen}]$

Algoritmo general:

1) Identificar variable aleatoria $Y$ que te interesa calcular.
2) Expresar $Y$ como suma de variables aleatorias indicadoras.
3) Aplicar linealidad de expectativas.

## Problema de Selección

Input: Array A con n distintos (para simplificar) números y un número $i\in \{1,2,...,n\}$.

Output: Elemento iésimo más pequeño de A.

Ejemplo: Mediana. $i=(n+1)/2$ para n impar, $i=n/2$ para n par.

### Reducción

Solucionar un problema en términos de otros problemas.

Reducción a sorting (Algoritmo $O(n \log n)$):
1) MergeSort
2) Devolver A[i]

Algoritmo $O(n)$ (aleatorio)

1) is $n=1$ devolver $A[1]$
2) Particionar()
3) si $p > i$, buscamos $i$ del lado izquierdo. Si $p < i$, buscamos $i - p$ del lado derecho. Si $p = i$, devolver p.

# Grafos y Algoritmo de Contracción

Un grafo posee:
- Vertices / nodos ($V$)
- Aristas / arcos / edges ($E$)
    - No dirigido (pares desordenados)
    - Dirigido (pares ordenados cabeza/cola) ("arcos")
- Corte de grafo: Un corte de un grafo $(V,E)$ es una partición de $V$ en dos conjuntos no-vacíos $A$ y $B$. Las aristas se dividen en tres: las que abarcan $A$ y $B$, etc. En un grafo dirigido, se dividen en cuatro (porque hay $A\rightarrow B$ y $B \rightarrow A$).
- Las aristas que cruzan un corte $(A,B)$ no-dirigido son todas las que tienen vertices en $A$ y $B$; las de un corte dirigido son sólo las que tienen cabeza en $A$ y cola en $B$.

## Problema del Corte Mínimo

Input: Grafo no-dirigido $G=(V,E)$ (aristas paralelas permitidas).

Meta: Computar un corte con el número menor posible de aristas cruzadas.

## Representaciones de Grafos

Sea $n$ el núm. de vertices, $m$ el núm. de aristas: en la mayoría de sus aplicaciones, m es al menos $\Omega (n)$ y como máximo $O(n^2)$. En un grafo disperso, m es $O(n)$ o cerca; en un grafo denso, m es $O(n^2)$ o cerca.

### Matriz de adyacencia

Representar a $G$ con una matriz binaria de $n\ x\ n$, donde $A_{ij}=1\Leftrightarrow\text{ G tiene una arista i-j}$. Su costo de espacio es cuadrático, por lo que es más optimo para grafos densamente poblados.

Variantes:

- $A_{ij}$ puede representar número de aristas (para contar paralelas)
- $A_{ij}$ puede ser el peso en un grafo pesado.
- $A_{ij}$ puede ser +1 para aristas $i\rightarrow j$, y -1 para lo opuesto.

### Listas de adyacencia

Espacio: $\Theta (m+n)$ (Es decir, $\Theta(max\{m,n\})$)

- Array/lista de vertices
- Array/lista de aristas
- Cada arista apunta a sus vertices (+ dirección si la hay)
- Cada vértice apunta a su aristas incidentes (si hay dirección, puede apuntar sólo a las colas)

## Algoritmo de Contracción Aleatoria (Karger)

1) Mientras haya más de dos vertices:
   - Elegir una arista $(u,v)$ restante al azar, de manera uniforme
   - Combinar ("Contraer") $u$ y $v$ en una sola arista (puede crear paralelos)
   - Remover bucles (self-loops)
2) Devolve el corte *representado* por los dos vertices finales.

Este algoritmo no siempre encuentra el MIN-CUT. Cuántas chances hay de que lo encuentre?